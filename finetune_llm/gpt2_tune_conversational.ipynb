{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661d2ba4",
   "metadata": {},
   "source": [
    "# Finetuning Base Model for Conversation\n",
    "\n",
    "- **Authors:** Riyaadh Gani and Damilola Ogunleye\n",
    "- **Project:** Food Recognition & Recipe LLM  \n",
    "- **Purpose:** Fine-tuning pipeline for our base model for conversational abilities\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook fine-tunes GPT-2 Medium on the processed OASST1 dataset with:\n",
    "- LoRA for parameter-efficient training\n",
    "- Multi-turn conversation support\n",
    "- RAG-ready architecture (context field preserved)\n",
    " \n",
    "**Requirements:**\n",
    "- Processed [OASST1 dataset](./data/processed/oasst1_multiturn_en)\n",
    "- GPU with at least 8GB VRAM (or use Colab/Kaggle)\n",
    "- Training_config.yaml for hyperparameter fine_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e889acc",
   "metadata": {},
   "source": [
    "### Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b00397",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade pip\n",
    "\n",
    "%pip install \"transformers==4.40.2\" \\\n",
    "             \"peft==0.11.1\" \\\n",
    "             \"accelerate==0.30.1\" \\\n",
    "             \"datasets==2.19.1\" \\\n",
    "             bitsandbytes \\\n",
    "             scipy \\\n",
    "             wandb \\\n",
    "             pyyaml \\\n",
    "             trl \\\n",
    "             safetensors \\\n",
    "             sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b83c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo already exists ‚Äî pulling latest changes...\n",
      "/content/DeepLearningLLM\n",
      "Already up to date.\n",
      "Working directory: /content/DeepLearningLLM\n",
      "Contents: ['.git', 'llm_data_preprocessing.ipynb', 'prepare_recipe_data_for_gpt2.py', 'wandb', 'finetune_llm', 'models', '.DS_Store', '.gitignore', 'lstm_model_training.ipynb', 'supportDocs']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "# Clone your repo into /content if not already present\n",
    "REPO_URL = \"https://github.com/Gani332/DeepLearningLLM.git\"\n",
    "REPO_PATH = \"/content/DeepLearningLLM\"\n",
    "\n",
    "# Clone repo only if it doesn't exist already\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    !git clone {REPO_URL} {REPO_PATH}\n",
    "else:\n",
    "    print(\"Repo already exists ‚Äî pulling latest changes...\")\n",
    "    %cd {REPO_PATH}\n",
    "    !git pull\n",
    "\n",
    "# Change working directory to your repo\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "# Show current working directory and its contents\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Contents:\", os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7cedd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "usingGoogleDrive = False  # Set to True if using Google Colab\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59d7fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if usingGoogleDrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    print(\"‚úì Google Drive mounted at /content/drive\")\n",
    "\n",
    "    # Base project directory in Google Drive\n",
    "    PROJECT_DIR = Path(\"/content/drive/MyDrive/cooking-assistant-project\")\n",
    "\n",
    "    # Verify directories exist\n",
    "    required_dirs = [\n",
    "        PROJECT_DIR / \"data\" / \"processed\" / \"oasst1_multiturn_en\" / \"train\",\n",
    "        PROJECT_DIR / \"data\" / \"processed\" / \"oasst1_multiturn_en\" / \"val\",\n",
    "    ]\n",
    "\n",
    "    print(\"Checking for processed data...\")\n",
    "    all_exist = True\n",
    "    for path in required_dirs:\n",
    "        exists = path.exists()\n",
    "        all_exist = all_exist and exists\n",
    "        print(f\"  {'‚úì' if exists else '‚úó'} {path}\")\n",
    "\n",
    "    if not all_exist:\n",
    "        print(\"\\n‚ö†Ô∏è ERROR: Processed data not found!\")\n",
    "        print(\"Please run the OASST1 processing notebook first.\")\n",
    "        raise FileNotFoundError(\"Processed data not found. Run OASST1 notebook first.\")\n",
    "\n",
    "    # Change working directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"\\n‚úì Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "592ec11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è NO GPU DETECTED!\")\n",
    "    print(\"  You MUST enable GPU for training:\")\n",
    "    print(\"     Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU ‚Üí Save\")\n",
    "    raise RuntimeError(\"GPU required for training. Please enable it in Runtime settings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07d66e",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "Only need to do once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6750410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2-medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Selected pretrained model\n",
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "print(f\"Downloading {model_name}...\")\n",
    "\n",
    "# loads the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # converts text to tokens\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # loads the pre-trained language model\n",
    "\n",
    "# Save locally for reuse\n",
    "tokenizer.save_pretrained(f\"./models/base/{model_name}\")\n",
    "model.save_pretrained(f\"./models/base/{model_name}\")\n",
    "print(\"Model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50309c9e",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee2db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents: ['OASST1', 'Cleaned']\n"
     ]
    }
   ],
   "source": [
    "print(\"Contents:\", os.listdir(\"/content/datasets/datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad95c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'dict'>\n",
      " Path to train data: /content/datasets/datasets/OASST1/processed/oasst1_multiturn_en/train\n",
      " Path to val data: /content/datasets/datasets/OASST1/processed/oasst1_multiturn_en/val\n"
     ]
    }
   ],
   "source": [
    "# Get the configuration parameters from the YAML file\n",
    "import yaml\n",
    "\n",
    "config_path = Path('finetune_llm/config/training_config.yaml')\n",
    "\n",
    "# Check if the file exists before trying to open it\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"The file {config_path} was not found. Please create config/training_config.yaml\")\n",
    "with open(config_path, 'r') as file:\n",
    "    # 1. Use safe_load() for security\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "    # The YAML file content is now a standard Python dictionary\n",
    "    print(f\"Loaded data type: {type(config)}\")\n",
    "    \n",
    "    # Check loaded properly\n",
    "    training_data_path = config['data']['train_data']\n",
    "    validation_data_path = config['data']['val_data']\n",
    "    \n",
    "    print(f\" Path to train data: {training_data_path}\")\n",
    "    print(f\" Path to val data: {validation_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f430d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train dataset: 5,139 examples\n",
      "‚úì Validation dataset: 572 examples\n",
      "\n",
      "Sample training example:\n",
      "======================================================================\n",
      "The following is a conversation between a user and a helpful cooking assistant.\n",
      "\n",
      "Previous conversation:\n",
      "User: Let's play a game of chess. I'll start:\n",
      "1. d4\n",
      "\n",
      "User: Let's play a game of chess. I'll start:\n",
      "1. d4\n",
      "Assistant: d5...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "train_path = Path(config[\"data\"][\"train_data\"])\n",
    "val_path = Path(config[\"data\"][\"val_data\"])\n",
    "\n",
    "# Check if datasets exist\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Training data not found at {train_path}.\\n\"\n",
    "        f\"Please run the OASST1 processing notebook first!\"\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_from_disk(str(train_path))\n",
    "val_dataset = load_from_disk(str(val_path))\n",
    "\n",
    "print(f\"‚úì Train dataset: {len(train_dataset):,} examples\")\n",
    "print(f\"‚úì Validation dataset: {len(val_dataset):,} examples\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(\"=\"*70)\n",
    "print(train_dataset[0]['text'][:500] + \"...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487d984",
   "metadata": {},
   "source": [
    "### Load the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49366393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./models/base/gpt2-medium...\n",
      "‚úì Tokenizer loaded\n",
      "  Vocab size: 50,257\n",
      "  Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {config['model']['base_model']}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model']['base_model'])\n",
    "\n",
    "# GPT-2 doesn't have a pad token, so we set it to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Pad on the left for generation\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"  Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01bb02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model...\n",
      "‚úì Model loaded\n",
      "  Total parameters: 354,823,168 (354.8M)\n",
      "  Trainable parameters: 354,823,168\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading model...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['model']['base_model'],\n",
    "    torch_dtype=torch.float16 if config['training']['fp16'] else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Get model size\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úì Model loaded\")\n",
    "print(f\"  Total parameters: {model_size:,} ({model_size/1e6:.1f}M)\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2b734",
   "metadata": {},
   "source": [
    "### Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4023e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,325,376 || all params: 359,148,544 || trainable%: 1.2043\n",
      "\n",
      "‚úì LoRA configured\n",
      "  Trainable parameters: 4,325,376 (4.33M)\n",
      "  Total parameters: 359,148,544 (359.1M)\n",
      "  Trainable %: 1.20%\n"
     ]
    }
   ],
   "source": [
    "if config['lora']['use_lora']:\n",
    "    print(\"Configuring LoRA...\")\n",
    "    \n",
    "    # LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config['lora']['lora_r'],\n",
    "        lora_alpha=config['lora']['lora_alpha'],\n",
    "        lora_dropout=config['lora']['lora_dropout'],\n",
    "        target_modules=config['lora']['target_modules'],\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training (memory efficient)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\n‚úì LoRA configured\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "    print(f\"  Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "    print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "else:\n",
    "    print(\"Training full model (no LoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84323a97",
   "metadata": {},
   "source": [
    "### Tokenise the Dataset\n",
    "\n",
    "Convert the string schema into something the model can actually understand and add the padding etc etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7103026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "  Tokenizing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a3699ff9904bfb905393957498b38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/5139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406018afdbf47e3887ba932c1662eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing val:   0%|          | 0/572 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenization complete\n",
      "  Train: 5,139 examples\n",
      "  Val: 572 examples\n",
      "\n",
      "Sample tokenized example:\n",
      "  Input IDs shape: 512\n",
      "  Attention mask shape: 512\n",
      "  Sample decoded: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text and prepare for causal language modeling.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=config['model']['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    # Labels = input_ids but mask padding positions with -100 so the loss ignores them\n",
    "    tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n",
    "    for i, mask in enumerate(tokenized[\"attention_mask\"]):\n",
    "        for j, m in enumerate(mask):\n",
    "            if m == 0:\n",
    "                tokenized[\"labels\"][i][j] = -100\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"  Tokenizing train dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "print(\"  Tokenizing validation dataset...\")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing val\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Tokenization complete\")\n",
    "print(f\"  Train: {len(tokenized_train):,} examples\")\n",
    "print(f\"  Val: {len(tokenized_val):,} examples\")\n",
    "\n",
    "# Show tokenized example (remember padding is on the left with <|endoftext|>)\n",
    "print(\"\\nSample tokenized example:\")\n",
    "sample = tokenized_train[0]\n",
    "print(f\"  Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"  Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "print(f\"  Sample decoded: {tokenizer.decode(sample['input_ids'][:100])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d92b22",
   "metadata": {},
   "source": [
    "### Set up the Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c927bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n",
      "‚úì Training arguments configured\n",
      "  Effective batch size: 16\n",
      "  Total training steps: ~963\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(config['training']['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Setting up training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=str(output_dir),\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=config['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    warmup_steps=config['training']['warmup_steps'],\n",
    "    optim=config['training']['optim'],\n",
    "    lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "\n",
    "    # Precision\n",
    "    fp16=config['training']['fp16'],\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=str(output_dir / \"logs\"),\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "\n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=config['training']['eval_steps'],\n",
    "\n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config['training']['save_steps'],\n",
    "    save_total_limit=config['training']['save_total_limit'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"wandb\" if config['training']['use_wandb'] else \"none\",\n",
    "    seed=42,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training arguments configured\")\n",
    "print(f\"  Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Total training steps: ~{len(tokenized_train) // (config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']) * config['training']['num_train_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08772871",
   "metadata": {},
   "source": [
    "### Initialise the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0913ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM \n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee9e1b",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e19013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriyaadh-gani2\u001b[0m (\u001b[33mriyaadh-gani2-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/DeepLearningLLM/wandb/run-20251115_164059-2y84ly61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant/runs/2y84ly61' target=\"_blank\">./models/base/gpt2-medium-3epochs</a></strong> to <a href='https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant' target=\"_blank\">https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant/runs/2y84ly61' target=\"_blank\">https://wandb.ai/riyaadh-gani2-ucl/gpt2-cooking-assistant/runs/2y84ly61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "Model: ./models/base/gpt2-medium\n",
      "Epochs: 3\n",
      "Training examples: 5,139\n",
      "Validation examples: 572\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='963' max='963' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [963/963 48:10, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.536200</td>\n",
       "      <td>2.318838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.948900</td>\n",
       "      <td>1.834234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.892600</td>\n",
       "      <td>1.784452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.830400</td>\n",
       "      <td>1.762046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.787100</td>\n",
       "      <td>1.750593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.788200</td>\n",
       "      <td>1.739409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.776000</td>\n",
       "      <td>1.736444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.801100</td>\n",
       "      <td>1.732749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.748800</td>\n",
       "      <td>1.732258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "======================================================================\n",
      "Training time: 2893.83 seconds\n",
      "Training samples/second: 5.33\n",
      "Final train loss: 2.1963\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if config['training']['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=config['training']['wandb_project'],\n",
    "        config=config,\n",
    "        name=f\"{config['model']['base_model']}-{config['training']['num_train_epochs']}epochs\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {config['model']['base_model']}\")\n",
    "print(f\"Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"Training examples: {len(tokenized_train):,}\")\n",
    "print(f\"Validation examples: {len(tokenized_val):,}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train and pray\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36c341",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4305e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143/143 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS\n",
      "======================================================================\n",
      "  eval_loss: 1.7323\n",
      "  eval_runtime: 38.8117\n",
      "  eval_samples_per_second: 14.7380\n",
      "  eval_steps_per_second: 3.6840\n",
      "  epoch: 2.9977\n",
      "======================================================================\n",
      "\n",
      " Perplexity: 5.65\n",
      "   (Lower is better - good models have perplexity < 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating model on validation set...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results['eval_loss'])\n",
    "print(f\"\\n Perplexity: {perplexity:.2f}\")\n",
    "print(\"   (Lower is better - good models have perplexity < 30)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95aa9d",
   "metadata": {},
   "source": [
    "### Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4ae2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final model...\n",
      "‚úì Model saved to: models/gpt2-conversational-v1/final\n",
      "‚úì Config saved to: models/gpt2-conversational-v1/final/training_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./models/base/gpt2-medium - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving final model...\")\n",
    "\n",
    "final_model_path = output_dir / \"final\"\n",
    "final_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "print(f\"‚úì Model saved to: {final_model_path}\")\n",
    "\n",
    "# Save training config as a json \n",
    "import json\n",
    "config_path = final_model_path / \"training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"‚úì Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaaf0ee",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12c8be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the fine-tuned model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting the fine-tuned model...\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(user_input, context=\"\", max_length=256, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate a response using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        user_input: User's question/message\n",
    "        context: Conversation history or retrieved information\n",
    "        max_length: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "    \"\"\"\n",
    "    # Format prompt (matches training format)\n",
    "    prompt = f\"\"\"The following is a conversation between a user and a helpful cooking assistant.\n",
    "\n",
    "{context}\n",
    "\n",
    "User: {user_input}\n",
    "Assistant:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"Assistant:\" in full_response:\n",
    "        response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        # Remove any trailing user prompts\n",
    "        if \"User:\" in response:\n",
    "            response = response.split(\"User:\")[0].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e139de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST RESPONSES\n",
      "======================================================================\n",
      "\n",
      "üî∏ Query 1: How do I make scrambled eggs?\n",
      "----------------------------------------------------------------------\n",
      "Response: There are several ways to make scrambled eggs. One of the most common\n",
      "\n",
      "\n",
      "üî∏ Query 2: What's the difference between baking and roasting?\n",
      "----------------------------------------------------------------------\n",
      "Response: The differences between baking and roasting can be confusing, but it can be very important to understand the difference between the two techniques to make sure you are getting the best results.\n",
      "\n",
      "Baking:\n",
      "\n",
      "Baking involves heating a mixture of ingredients, such as flour, sugar, and salt, and then slowly adding water and baking the mixture at a high temperature. The resulting product is a cake or cookies that is moist and soft, but not dense or dense.\n",
      "\n",
      "Roasting:\n",
      "\n",
      "Roasting involves the addition of heat and moisture to a pan, creating a dense, soft, and chewy product that is ready to eat.\n",
      "\n",
      "The two methods are closely related, but they have their own unique characteristics that are important to consider when choosing the right oven for your baking or roasting project.\n",
      "\n",
      "Baking:\n",
      "\n",
      "Baking involves heating a mixture of ingredients, such as flour, sugar, and salt, and then slowly adding water and baking the mixture at a high temperature. The resulting product is a cake or cookies that is moist and soft, but not dense or dense.\n",
      "\n",
      "Roasting:\n",
      "\n",
      "Roasting involves the addition of heat and moisture to a pan, creating a dense, soft, and chewy product that is ready\n",
      "\n",
      "\n",
      "üî∏ Query 3: Can you explain what saut√©ing means?\n",
      "----------------------------------------------------------------------\n",
      "Response: There are many different types of saut√©ing dishes, but here are some common types of saut√©ing dishes:\n",
      "\n",
      "1. Spicy Saut√©ed Chicken: A spicy chicken saut√©ed with garlic and onions can be served with rice or noodles.\n",
      "2. Spicy Chicken Saut√©ed with Mushrooms and Rice: A spicy chicken saut√©ed\n",
      "\n",
      "\n",
      "üî∏ Query 4: My cake collapsed in the middle. What went wrong?\n",
      "----------------------------------------------------------------------\n",
      "Response: If your cake collapsed in the middle, there may be a number of possible causes:\n",
      "\n",
      "1. The pan you used for baking the cake may have been improperly cleaned.\n",
      "2. The temperature of the oven may have been too hot for the cake to bake properly.\n",
      "3. The oven may have been improperly calibrated.\n",
      "4. The oven may have been improperly cleaned.\n",
      "5. The oven may have been improperly calibrated.\n",
      "6. The oven may have been improperly cleaned.\n",
      "7. The oven may have been improperly calibrated.\n",
      "8. The oven may have been improperly cleaned.\n",
      "9. The oven may have been improperly cleaned.\n",
      "10. The oven may have been improperly calibrated.\n",
      "11. The oven may have been improperly cleaned.\n",
      "12. The oven may have been improperly calibrated.\n",
      "13. The oven may have been improperly cleaned.\n",
      "14. The oven may have been improperly cleaned.\n",
      "15. The oven may have been improperly cleaned.\n",
      "16. The oven may have been improperly cleaned.\n",
      "17. The oven may have been improperly cleaned.\n",
      "18. The oven may have been improperly cleaned.\n",
      "19. The oven may have been improperly cleaned.\n",
      "20. The oven may have been improperly cleaned.\n",
      "21.\n",
      "\n",
      "\n",
      "üî∏ Query 5: How do I know when pasta is done?\n",
      "----------------------------------------------------------------------\n",
      "Response: It depends on how long pasta has been cooked. It's usually done in the oven or on a griddle. You can check the cooking time with a pasta thermometer.\n",
      "\n",
      "\n",
      "For pasta that has been cooked for a long time, it can be difficult to tell when it's finished. The pasta will be slightly tacky and it may have a little bit of liquid on it.\n",
      "\n",
      "\n",
      "To check the cooking time of pasta, simply turn the pasta upside down and take a long, sharp knife and cut through the pasta into the desired thickness.\n",
      "\n",
      "If the pasta is still tacky when you cut it, it's finished cooking.\n",
      "\n",
      "\n",
      "For pasta that has been cooked for a short time, it may not be noticeable that the pasta has finished cooking. It may be more noticeable that the pasta is cooked through and the sauce is thickened.\n",
      "\n",
      "To check the cooking time of pasta, simply turn the pasta upside down and take a long, sharp knife and cut through the pasta into the desired thickness.\n",
      "\n",
      "If the pasta is still tacky when you cut it, it's finished cooking.\n",
      "\n",
      "\n",
      "To check the cooking time of pasta, simply turn the pasta upside down and take a long, sharp knife and cut through the pasta into the desired\n",
      "\n",
      "\n",
      "üî∏ Query 6: I have eggs, tomatoes, and spinach. What can I cook for breakfast?\n",
      "----------------------------------------------------------------------\n",
      "Response: Eggs, tomatoes, and spinach are a great way to prepare breakfast for your family. You can cook them in a crockpot or microwave to make them easier to eat and more nutritious.\n",
      "\n",
      "Tomatoes and spinach are a great source of fiber, vitamins, minerals, and potassium. They're also a great source of protein, fiber, and potassium. They're also a good source of antioxidants and vitamins.\n",
      "\n",
      "Here are some ideas for breakfast:\n",
      "\n",
      "1. Eggs: Cook eggs in a crockpot or microwave to make them easier to eat and more nutritious. They're also a good source of protein, fiber, and potassium.\n",
      "\n",
      "2. Tomatoes: Cut up tomatoes and mix them with yogurt and salt to make a smoothie. You can also cook the tomatoes in a crockpot or microwave to make them easier to eat.\n",
      "\n",
      "3. Spinach: Cut up spinach and mix it with yogurt and salt to make a smoothie. You can also cook the spinach in a crockpot or microwave to make them easier to eat.\n",
      "\n",
      "4. Oatmeal: You can add oats to your oatmeal to help reduce sugar and fiber. Oats are a great source of iron, potassium, and protein.\n",
      "\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with sample queries\n",
    "\n",
    "test_queries = [\n",
    "    \"How do I make scrambled eggs?\",\n",
    "    \"What's the difference between baking and roasting?\",\n",
    "    \"Can you explain what saut√©ing means?\",\n",
    "    \"My cake collapsed in the middle. What went wrong?\",\n",
    "    \"How do I know when pasta is done?\",\n",
    "    \"I have eggs, tomatoes, and spinach. What can I cook for breakfast?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST RESPONSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüî∏ Query {i}: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "    response = generate_response(query)\n",
    "    print(f\"Response: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f0cf8",
   "metadata": {},
   "source": [
    "### Test Multi-Turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816f236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MULTI-TURN CONVERSATION TEST\n",
      "======================================================================\n",
      "\n",
      "üë§ User: How do I make risotto?\n",
      "ü§ñ Assistant: To make risotto, you can use a heavy-bottomed pot, a heavy-bottomed saucepan, and a heavy-bottomed griddle. To make risotto, you can use a heavy-bottomed pot, a heavy-bottomed saucepan, and a heavy-bottomed griddle.\n",
      "\n",
      "Step 1: Preheat the oven to 375¬∞F.\n",
      "Step 2: In a medium saucepan, combine the flour, salt, and pepper. Stir in the olive oil and cook, stirring frequently, until the mixture is smooth and thickened, about 5 minutes.\n",
      "Step 3: Add the mushrooms, stir to combine, and cook, stirring occasionally, until the mushrooms are tender, about 5 minutes.\n",
      "Step 4: Add the remaining ingredients, stirring to combine, and cook, stirring occasionally, until the mixture is thickened, about 5 minutes.\n",
      "Step 5: Add the rice, stir to combine, and cook, stirring occasionally, until the rice is cooked through, about 5 minutes.\n",
      "Step 6: Add the spinach, stir to combine, and cook, stirring occasionally, until the spinach is wilted, about 5 minutes.\n",
      "Step 7: Add the remaining ingredients, stirring to combine, and cook, stirring occasionally,\n",
      "\n",
      "üë§ User: What type of rice should I use?\n",
      "ü§ñ Assistant: If you want to make risotto with a light, thick, rice, you can use:\n",
      "\n",
      "‚Ä¢ Rice from rice paddies, which are planted near the water.\n",
      "‚Ä¢ White rice, which is a type of brown rice.\n",
      "‚Ä¢ Brown rice from rice paddies, which are planted near the water.\n",
      "‚Ä¢ White rice from green rice, which is a type of white rice.\n",
      "\n",
      "‚Ä¢ Green rice from rice paddies, which is a type of green rice.\n",
      "\n",
      "‚Ä¢ Red rice, which is a type of brown rice.\n",
      "\n",
      "‚Ä¢ Red rice from green rice, which is a type of red rice.\n",
      "\n",
      "Step 1: Add the rice and the other ingredients to the saucepan. Bring to a boil, reduce the heat, and simmer for 5 minutes.\n",
      "Step 2: Meanwhile, preheat the oven to 350¬∞F.\n",
      "Step 3: In a large mixing bowl, combine the egg yolks, the sesame oil, and the sugar. Add the rice mixture to the egg yolks, whisking constantly, until the rice is thickened and smooth, about 5 minutes.\n",
      "Step 4: Stir in the remaining ingredients, stirring to combine, and cook, stirring occasionally, until the rice is thickened\n",
      "\n",
      "üë§ User: How long does it take?\n",
      "ü§ñ Assistant: The best way to cook the rice is by using a heavy-bottomed saucepan, a heavy\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-TURN CONVERSATION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "conversation_history = []\n",
    "\n",
    "turn_1_input = \"How do I make risotto?\"\n",
    "print(f\"\\nüë§ User: {turn_1_input}\")\n",
    "\n",
    "# First turn (no history)\n",
    "response_1 = generate_response(turn_1_input, context=\"\")\n",
    "print(f\"ü§ñ Assistant: {response_1}\")\n",
    "\n",
    "# Add to history\n",
    "conversation_history.append(f\"User: {turn_1_input}\")\n",
    "conversation_history.append(f\"Assistant: {response_1}\")\n",
    "\n",
    "# Second turn (with history)\n",
    "turn_2_input = \"What type of rice should I use?\"\n",
    "context = \"Previous conversation:\\n\" + \"\\n\".join(conversation_history)\n",
    "\n",
    "print(f\"\\nüë§ User: {turn_2_input}\")\n",
    "response_2 = generate_response(turn_2_input, context=context)\n",
    "print(f\"ü§ñ Assistant: {response_2}\")\n",
    "\n",
    "conversation_history.append(f\"User: {turn_2_input}\")\n",
    "conversation_history.append(f\"Assistant: {response_2}\")\n",
    "\n",
    "# Third turn\n",
    "turn_3_input = \"How long does it take?\"\n",
    "context = \"Previous conversation:\\n\" + \"\\n\".join(conversation_history)\n",
    "\n",
    "print(f\"\\nüë§ User: {turn_3_input}\")\n",
    "response_3 = generate_response(turn_3_input, context=context)\n",
    "print(f\"ü§ñ Assistant: {response_3}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd98c1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70ec1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Successfully fine-tuned ./models/base/gpt2-medium\n",
      "‚úÖ Final validation perplexity: 5.65\n",
      "‚úÖ Model saved to: models/gpt2-conversational-v1/final\n",
      "\n",
      "üìÅ Files:\n",
      "  Model weights: models/gpt2-conversational-v1/final\n",
      "  Tokenizer: models/gpt2-conversational-v1/final\n",
      "  Training config: models/gpt2-conversational-v1/final/training_config.json\n",
      "  Checkpoints: models/gpt2-conversational-v1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Successfully fine-tuned {config['model']['base_model']}\")\n",
    "print(f\"‚úÖ Final validation perplexity: {perplexity:.2f}\")\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ Files:\")\n",
    "print(f\"  Model weights: {final_model_path}\")\n",
    "print(f\"  Tokenizer: {final_model_path}\")\n",
    "print(f\"  Training config: {config_path}\")\n",
    "print(f\"  Checkpoints: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
