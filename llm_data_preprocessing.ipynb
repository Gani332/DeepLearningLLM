{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Data Cleaning & Preparation Pipeline\n",
    "\n",
    "- **Authors:** Riyaadh Gani and Damilola Ogunleye\n",
    "- **Project:** Food Recognition & Recipe LLM  \n",
    "- **Purpose:** Clean and prepare recipe data for embedding generation and model training\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook processes two major recipe datasets:\n",
    "- **Food.com**: 231K recipes with reviews and nutrition data\n",
    "- **RecipeNLG**: 2.2M recipes (500K sample used)\n",
    "\n",
    "**Output:** 3 datasets ready for training:\n",
    "\n",
    "1. `nutrition_lookup.csv`\n",
    "- **Purpose:** Reference table for nutritional information\n",
    "- **Usage:** Lookup table (not for training)\n",
    "- **Rows:** ~231K nutrition entries\n",
    "\n",
    "2. `clean_recipes.csv`\n",
    "- **Purpose:** Single-turn recipe training data\n",
    "- **Usage:** Baseline LSTM training\n",
    "- **Rows:** ~4-5M prompt-response pairs\n",
    "- **Format:** `prompt, response`\n",
    "\n",
    "3. `conversational_training_data.csv`\n",
    "- **Purpose:** Multi-turn conversational training data\n",
    "- **Usage:** Conversational LSTM training\n",
    "- **Rows:** ~10K-50K conversation pairs\n",
    "- **Format:** `input, output` (with conversation history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable progress bars for pandas operations\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "  - Output path: datasets/Cleaned/clean_recipes.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "FOOD_COM_PATH = \"datasets/kaggleFood\"\n",
    "RECIPENLG_PATH = \"datasets/recipeNLG/RecipeNLG_dataset.csv\"\n",
    "OUTPUT_PATH = \"datasets/Cleaned/clean_recipes.csv\"\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  - Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "[1/3] Loading Food.com recipes...\n",
      "      ‚úì Loaded 231,637 recipes\n",
      "[2/3] Loading Food.com interactions...\n",
      "      ‚úì Loaded 1,132,367 interactions\n",
      "[3/3] Loading RecipeNLG dataset...\n",
      "      ‚úì Loaded 2,231,142 recipes\n",
      "\n",
      "‚úì All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load Food.com recipes\n",
    "print(\"[1/3] Loading Food.com recipes...\")\n",
    "food_com_recipes = pd.read_csv(f\"{FOOD_COM_PATH}/RAW_recipes.csv\")\n",
    "print(f\"      ‚úì Loaded {len(food_com_recipes):,} recipes\")\n",
    "\n",
    "# Load Food.com interactions (reviews)\n",
    "print(\"[2/3] Loading Food.com interactions...\")\n",
    "food_com_interactions = pd.read_csv(f\"{FOOD_COM_PATH}/RAW_interactions.csv\")\n",
    "print(f\"      ‚úì Loaded {len(food_com_interactions):,} interactions\")\n",
    "\n",
    "# Load RecipeNLG\n",
    "print(f\"[3/3] Loading RecipeNLG dataset...\")\n",
    "recipenlg_df = pd.read_csv(RECIPENLG_PATH)\n",
    "print(f\"      ‚úì Loaded {len(recipenlg_df):,} recipes\")\n",
    "\n",
    "print(\"\\n‚úì All datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schemas:\n",
      "\n",
      "Food.com Recipes:\n",
      "  Columns: ['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags', 'nutrition', 'n_steps', 'steps', 'description', 'ingredients', 'n_ingredients']\n",
      "  Shape: (231637, 12)\n",
      "\n",
      "RecipeNLG:\n",
      "  Columns: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source', 'NER']\n",
      "  Shape: (2231142, 7)\n",
      "\n",
      "Food.com Interactions:\n",
      "  Columns: ['user_id', 'recipe_id', 'date', 'rating', 'review']\n",
      "  Shape: (1132367, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Schemas:\\n\")\n",
    "print(\"Food.com Recipes:\")\n",
    "print(f\"  Columns: {food_com_recipes.columns.tolist()}\")\n",
    "print(f\"  Shape: {food_com_recipes.shape}\\n\")\n",
    "\n",
    "print(\"RecipeNLG:\")\n",
    "print(f\"  Columns: {recipenlg_df.columns.tolist()}\")\n",
    "print(f\"  Shape: {recipenlg_df.shape}\\n\")\n",
    "\n",
    "print(\"Food.com Interactions:\")\n",
    "print(f\"  Columns: {food_com_interactions.columns.tolist()}\")\n",
    "print(f\"  Shape: {food_com_interactions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Food.com Recipe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>31490</td>\n",
       "      <td>30</td>\n",
       "      <td>26278</td>\n",
       "      <td>2002-06-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "1            a bit different  breakfast pizza   31490       30   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "1           26278  2002-06-17   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "1  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "\n",
       "                                   nutrition  n_steps  \\\n",
       "0      [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "1  [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "\n",
       "                                         ingredients  n_ingredients  \n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...              6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample RecipeNLG Recipe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  title  \\\n",
       "0           0    No-Bake Nut Cookies   \n",
       "1           1  Jewell Ball'S Chicken   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample recipes\n",
    "print(\"Sample Food.com Recipe:\\n\")\n",
    "display(food_com_recipes.head(2))\n",
    "\n",
    "print(\"\\nSample RecipeNLG Recipe:\\n\")\n",
    "display(recipenlg_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutrition Lookup Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 231,637 raw Food.com recipes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231637/231637 [00:01<00:00, 123623.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed nutrition info for 230,133 recipes\n",
      "üíæ Nutrition lookup saved to: datasets/Cleaned/nutrition_lookup.csv\n",
      "Sample rows:\n",
      "             id                                          title  calories  \\\n",
      "195584  444694       spicy chicken with carrot and herb salad     424.3   \n",
      "91146    40313                           georgia peach cooler     310.7   \n",
      "101861  148628          ham with pineapple orange dijon glaze     150.3   \n",
      "154664  209173         paula deen s easy squeeze honey butter    2703.8   \n",
      "142423  471871  na me inspired roasted squash  vegan friendly     818.9   \n",
      "\n",
      "        protein_g  carbs_g  fat_g  sat_fat_g  sugar_g  sodium_mg  \n",
      "195584       57.0     22.0    8.0        7.0     30.0        7.0  \n",
      "91146        19.0     17.0   14.0       28.0    130.0        5.0  \n",
      "101861        0.0     12.0    0.0        0.0    150.0        1.0  \n",
      "154664        6.0     24.0  421.0      238.0    278.0      133.0  \n",
      "142423       23.0     50.0   43.0      119.0    169.0       26.0  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# === Load raw Food.com data (from Kaggle dataset) ===\n",
    "RAW_FOOD_PATH = \"datasets/kaggleFood/RAW_recipes.csv\"\n",
    "raw_food = pd.read_csv(RAW_FOOD_PATH)\n",
    "\n",
    "print(f\"Loaded {len(raw_food):,} raw Food.com recipes\")\n",
    "\n",
    "# === Parse the nutrition column ===\n",
    "# Each nutrition entry looks like: [138.4, 4.0, 5.0, 20.0, 10.0, 15.0, 3.0]\n",
    "# Format: [calories, total_fat(g), sugar(g), sodium(mg), protein(g), sat_fat(g), carbs(g)]\n",
    "\n",
    "def parse_nutrition(entry):\n",
    "    \"\"\"Convert Food.com nutrition string into a dict of numeric fields.\"\"\"\n",
    "    try:\n",
    "        vals = ast.literal_eval(entry)\n",
    "        return {\n",
    "            \"calories\": float(vals[0]),\n",
    "            \"fat_g\": float(vals[1]),\n",
    "            \"sugar_g\": float(vals[2]),\n",
    "            \"sodium_mg\": float(vals[3]),\n",
    "            \"protein_g\": float(vals[4]),\n",
    "            \"sat_fat_g\": float(vals[5]),\n",
    "            \"carbs_g\": float(vals[6])\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Apply parsing\n",
    "nutrition_parsed = raw_food[\"nutrition\"].progress_apply(parse_nutrition)\n",
    "nutrition_df = pd.DataFrame(list(nutrition_parsed))\n",
    "\n",
    "# Attach IDs and titles for lookup\n",
    "nutrition_df[\"id\"] = raw_food[\"id\"]\n",
    "nutrition_df[\"title\"] = raw_food[\"name\"].str.lower().str.strip()\n",
    "\n",
    "# === Clean and validate ===\n",
    "nutrition_df = nutrition_df.dropna(subset=[\"calories\", \"protein_g\", \"carbs_g\"])\n",
    "nutrition_df = nutrition_df[nutrition_df[\"calories\"] > 0]\n",
    "nutrition_df = nutrition_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "# Reorder columns neatly\n",
    "nutrition_df = nutrition_df[\n",
    "    [\"id\", \"title\", \"calories\", \"protein_g\", \"carbs_g\", \"fat_g\", \"sat_fat_g\", \"sugar_g\", \"sodium_mg\"]\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Parsed nutrition info for {len(nutrition_df):,} recipes\")\n",
    "\n",
    "# === Save to CSV for lookup ===\n",
    "NUTRITION_OUTPUT_PATH = \"datasets/Cleaned/nutrition_lookup.csv\"\n",
    "nutrition_df.to_csv(NUTRITION_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"üíæ Nutrition lookup saved to: {NUTRITION_OUTPUT_PATH}\")\n",
    "print(\"Sample rows:\\n\", nutrition_df.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Define Cleaning Functions\n",
    "\n",
    "These functions handle:\n",
    "- Text normalization (lowercase, whitespace)\n",
    "- HTML/URL removal\n",
    "- List parsing and formatting\n",
    "- Special character handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cleaning functions defined!\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing HTML, URLs, and normalizing whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string to clean\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned lowercase string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)  # Remove HTML tags\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?\\[\\]\\(\\)\\-'\\\"]\", \"\", text)  # Keep essential punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_list_string(s):\n",
    "    \"\"\"\n",
    "    Safely parse string representation of Python lists.\n",
    "    \n",
    "    Args:\n",
    "        s: String representation of a list (e.g., \"['item1', 'item2']\")\n",
    "    \n",
    "    Returns:\n",
    "        Parsed list or empty list if parsing fails\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def format_ingredients(ingredients):\n",
    "    \"\"\"\n",
    "    Format ingredients list into a comma-separated string.\n",
    "    \n",
    "    Args:\n",
    "        ingredients: List or string representation of ingredients\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string (e.g., \"flour, sugar, eggs\")\n",
    "    \"\"\"\n",
    "    if isinstance(ingredients, str):\n",
    "        ingredients = parse_list_string(ingredients)\n",
    "    \n",
    "    if isinstance(ingredients, list):\n",
    "        return \", \".join([str(i).strip() for i in ingredients if i])\n",
    "    \n",
    "    return str(ingredients)\n",
    "\n",
    "\n",
    "def format_directions(directions):\n",
    "    \"\"\"\n",
    "    Format cooking directions with numbered steps.\n",
    "    \n",
    "    Args:\n",
    "        directions: List or string representation of cooking steps\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with numbered steps (e.g., \"1. preheat oven 2. mix ingredients\")\n",
    "    \"\"\"\n",
    "    if isinstance(directions, str):\n",
    "        directions = parse_list_string(directions)\n",
    "    \n",
    "    if isinstance(directions, list):\n",
    "        return \" \".join([f\"{i+1}. {str(step).strip()}\" for i, step in enumerate(directions) if step])\n",
    "    \n",
    "    return str(directions)\n",
    "\n",
    "\n",
    "print(\"‚úì Cleaning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Why We Clean and Normalize Text\n",
    "\n",
    "Before any deep learning model can learn meaningful patterns, the input data must be **consistent, noise-free, and normalized**.  \n",
    "This section applies the same principles we learned in COMP0220 about *input normalization for stable training*.\n",
    "\n",
    "#### 1. Lowercasing & Whitespace Normalization\n",
    "- Ensures that \"Milk\" and \"milk\" are treated as the same token, reducing vocabulary size and data sparsity.  \n",
    "- Makes gradient updates more stable because the model doesn‚Äôt waste capacity learning redundant word forms.\n",
    "\n",
    "#### 2. HTML / URL Removal\n",
    "- Removes irrelevant tokens (e.g., `<div>`, `http://...`) that carry no semantic meaning for recipes or ingredients.  \n",
    "- Prevents the tokenizer and embedding layers from assigning random weights to non-informative symbols.\n",
    "\n",
    "#### 3. List Parsing & Formatting\n",
    "- Converts ingredient lists or numbered steps into clean, uniform strings so the model can learn true relationships  \n",
    "  between **ingredients ‚Üí actions ‚Üí outcomes** instead of formatting artifacts.\n",
    "\n",
    "#### 4. Special Character Handling\n",
    "- Normalizes punctuation, removes emojis or stray symbols that increase token noise.  \n",
    "- Keeps the text distribution consistent, improving embedding quality and convergence speed.\n",
    "\n",
    "In short: **clean, normalized text ‚Üí cleaner embeddings ‚Üí faster convergence ‚Üí better generalization.**  \n",
    "This mirrors the data-normalization step used in CNNs (e.g., scaling pixel values) but applied here to language data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Process Food.com Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Food.com recipes...\n",
      "\n",
      "[1/4] Cleaning titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231637/231637 [00:00<00:00, 358934.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/4] Cleaning descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231637/231637 [00:02<00:00, 110119.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/4] Formatting ingredients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231637/231637 [00:02<00:00, 84505.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/4] Formatting directions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231637/231637 [00:03<00:00, 67341.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Food.com processing complete!\n",
      "  - Recipes retained: 231,635 / 231,637\n",
      "  - Removal rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Food.com recipes...\\n\")\n",
    "\n",
    "# Select and rename columns\n",
    "food_com_clean = food_com_recipes[[\"id\", \"name\", \"ingredients\", \"steps\", \"description\"]].copy()\n",
    "food_com_clean.rename(columns={\"name\": \"title\", \"steps\": \"directions\"}, inplace=True)\n",
    "\n",
    "# Clean text fields\n",
    "print(\"[1/4] Cleaning titles...\")\n",
    "food_com_clean[\"title\"] = food_com_clean[\"title\"].progress_apply(clean_text)\n",
    "\n",
    "print(\"[2/4] Cleaning descriptions...\")\n",
    "food_com_clean[\"description\"] = food_com_clean[\"description\"].progress_apply(clean_text)\n",
    "\n",
    "print(\"[3/4] Formatting ingredients...\")\n",
    "food_com_clean[\"ingredients\"] = food_com_clean[\"ingredients\"].progress_apply(format_ingredients)\n",
    "food_com_clean[\"ingredients\"] = food_com_clean[\"ingredients\"].apply(clean_text)\n",
    "\n",
    "print(\"[4/4] Formatting directions...\")\n",
    "food_com_clean[\"directions\"] = food_com_clean[\"directions\"].progress_apply(format_directions)\n",
    "food_com_clean[\"directions\"] = food_com_clean[\"directions\"].apply(clean_text)\n",
    "\n",
    "# Remove invalid entries\n",
    "initial_count = len(food_com_clean)\n",
    "food_com_clean = food_com_clean.dropna(subset=[\"title\", \"ingredients\", \"directions\"])\n",
    "food_com_clean = food_com_clean[\n",
    "    (food_com_clean[\"title\"].str.len() > 0) &\n",
    "    (food_com_clean[\"ingredients\"].str.len() > 0) &\n",
    "    (food_com_clean[\"directions\"].str.len() > 0)\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úì Food.com processing complete!\")\n",
    "print(f\"  - Recipes retained: {len(food_com_clean):,} / {initial_count:,}\")\n",
    "print(f\"  - Removal rate: {(1 - len(food_com_clean)/initial_count)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Process Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Food.com reviews...\n",
      "\n",
      "[1/2] Cleaning review text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1132367/1132367 [00:15<00:00, 74661.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Merging reviews with recipes...\n",
      "\n",
      "‚úì Reviews processed!\n",
      "  - Recipes with reviews: 231,236 / 231,635\n",
      "  - Coverage: 99.8%\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Food.com reviews...\\n\")\n",
    "\n",
    "# Clean review text\n",
    "print(\"[1/2] Cleaning review text...\")\n",
    "food_com_interactions[\"review\"] = food_com_interactions[\"review\"].progress_apply(clean_text)\n",
    "\n",
    "# Filter out very short reviews\n",
    "food_com_interactions = food_com_interactions[food_com_interactions[\"review\"].str.len() > 10]\n",
    "\n",
    "# Get first review for each recipe\n",
    "print(\"[2/2] Merging reviews with recipes...\")\n",
    "first_reviews = food_com_interactions.groupby(\"recipe_id\")[\"review\"].first().reset_index()\n",
    "\n",
    "food_com_with_reviews = pd.merge(\n",
    "    food_com_clean,\n",
    "    first_reviews,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"recipe_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "review_count = food_com_with_reviews[\"review\"].notna().sum()\n",
    "print(f\"\\n‚úì Reviews processed!\")\n",
    "print(f\"  - Recipes with reviews: {review_count:,} / {len(food_com_with_reviews):,}\")\n",
    "print(f\"  - Coverage: {(review_count/len(food_com_with_reviews))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Process RecipeNLG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RecipeNLG recipes...\n",
      "\n",
      "[1/3] Cleaning titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2231142/2231142 [00:05<00:00, 387785.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] Formatting ingredients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2231142/2231142 [00:27<00:00, 82453.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/3] Formatting directions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2231142/2231142 [00:30<00:00, 73136.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì RecipeNLG processing complete!\n",
      "  - Recipes retained: 2,231,129 / 2,231,142\n",
      "  - Removal rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing RecipeNLG recipes...\\n\")\n",
    "\n",
    "# Select relevant columns\n",
    "recipenlg_clean = recipenlg_df[[\"title\", \"ingredients\", \"directions\"]].copy()\n",
    "\n",
    "# Clean text fields\n",
    "print(\"[1/3] Cleaning titles...\")\n",
    "recipenlg_clean[\"title\"] = recipenlg_clean[\"title\"].progress_apply(clean_text)\n",
    "\n",
    "print(\"[2/3] Formatting ingredients...\")\n",
    "recipenlg_clean[\"ingredients\"] = recipenlg_clean[\"ingredients\"].progress_apply(format_ingredients)\n",
    "recipenlg_clean[\"ingredients\"] = recipenlg_clean[\"ingredients\"].apply(clean_text)\n",
    "\n",
    "print(\"[3/3] Formatting directions...\")\n",
    "recipenlg_clean[\"directions\"] = recipenlg_clean[\"directions\"].progress_apply(format_directions)\n",
    "recipenlg_clean[\"directions\"] = recipenlg_clean[\"directions\"].apply(clean_text)\n",
    "\n",
    "# Remove invalid entries\n",
    "initial_count = len(recipenlg_clean)\n",
    "recipenlg_clean = recipenlg_clean.dropna(subset=[\"title\", \"ingredients\", \"directions\"])\n",
    "recipenlg_clean = recipenlg_clean[\n",
    "    (recipenlg_clean[\"title\"].str.len() > 0) &\n",
    "    (recipenlg_clean[\"ingredients\"].str.len() > 0) &\n",
    "    (recipenlg_clean[\"directions\"].str.len() > 0)\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úì RecipeNLG processing complete!\")\n",
    "print(f\"  - Recipes retained: {len(recipenlg_clean):,} / {initial_count:,}\")\n",
    "print(f\"  - Removal rate: {(1 - len(recipenlg_clean)/initial_count)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Create Prompt-Response Pairs\n",
    "\n",
    "We create three types of conversational pairs:\n",
    "1. **Ingredient-based**: \"I have X ingredients, what can I make?\"\n",
    "2. **Recipe request**: \"How do I make X?\"\n",
    "3. **Review inquiry**: \"What do people think about X?\" (Food.com only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prompt-response pairs...\n",
      "\n",
      "[1/2] Processing Food.com recipes (with reviews)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231635/231635 [00:08<00:00, 27858.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úì Created 694,506 pairs\n",
      "[2/2] Processing RecipeNLG recipes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2231129/2231129 [01:03<00:00, 35220.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úì Created 4,462,258 pairs\n",
      "\n",
      "‚úì Total pairs created: 5,156,764\n"
     ]
    }
   ],
   "source": [
    "def create_recipe_pairs(df, include_reviews=False):\n",
    "    \"\"\"\n",
    "    Generate prompt-response pairs from recipe data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns [title, ingredients, directions, review (optional)]\n",
    "        include_reviews: Whether to create review-based pairs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns [prompt, response]\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating pairs\"):\n",
    "        # Type 1: Ingredient-based query\n",
    "        prompt1 = f\"i have these ingredients: {row['ingredients']}. what can i make?\"\n",
    "        response1 = f\"you could make {row['title']}. here are the instructions: {row['directions']}\"\n",
    "        prompts.append(prompt1)\n",
    "        responses.append(response1)\n",
    "        \n",
    "        # Type 2: Recipe name query\n",
    "        prompt2 = f\"how do i make {row['title']}?\"\n",
    "        response2 = f\"to make {row['title']}, you'll need: {row['ingredients']}. then follow these steps: {row['directions']}\"\n",
    "        prompts.append(prompt2)\n",
    "        responses.append(response2)\n",
    "        \n",
    "        # Type 3: Review query (if available)\n",
    "        if include_reviews and \"review\" in df.columns and pd.notna(row.get(\"review\")):\n",
    "            prompt3 = f\"what do people think about {row['title']}?\"\n",
    "            response3 = row[\"review\"]\n",
    "            prompts.append(prompt3)\n",
    "            responses.append(response3)\n",
    "    \n",
    "    return pd.DataFrame({\"prompt\": prompts, \"response\": responses})\n",
    "\n",
    "\n",
    "print(\"Creating prompt-response pairs...\\n\")\n",
    "\n",
    "print(\"[1/2] Processing Food.com recipes (with reviews)...\")\n",
    "food_com_pairs = create_recipe_pairs(food_com_with_reviews, include_reviews=True)\n",
    "print(f\"      ‚úì Created {len(food_com_pairs):,} pairs\")\n",
    "\n",
    "print(\"[2/2] Processing RecipeNLG recipes...\")\n",
    "recipenlg_pairs = create_recipe_pairs(recipenlg_clean, include_reviews=False)\n",
    "print(f\"      ‚úì Created {len(recipenlg_pairs):,} pairs\")\n",
    "\n",
    "print(f\"\\n‚úì Total pairs created: {len(food_com_pairs) + len(recipenlg_pairs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Combine & Finalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining and finalizing dataset...\n",
      "\n",
      "[1/5] Combined datasets: 5,156,764 total pairs\n",
      "[2/5] After dropping NaN: 5,156,764 pairs\n",
      "[3/5] After dropping duplicates: 5,156,758 pairs\n",
      "[4/5] After filtering short responses: 5,155,414 pairs\n",
      "[5/5] Dataset shuffled\n",
      "\n",
      "‚úì Final dataset ready with 5,155,414 prompt-response pairs!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining and finalizing dataset...\\n\")\n",
    "\n",
    "# Combine all pairs\n",
    "final_df = pd.concat([food_com_pairs, recipenlg_pairs], ignore_index=True)\n",
    "print(f\"[1/5] Combined datasets: {len(final_df):,} total pairs\")\n",
    "\n",
    "# Remove missing values\n",
    "final_df = final_df.dropna()\n",
    "print(f\"[2/5] After dropping NaN: {len(final_df):,} pairs\")\n",
    "\n",
    "# Remove duplicates\n",
    "final_df = final_df.drop_duplicates()\n",
    "print(f\"[3/5] After dropping duplicates: {len(final_df):,} pairs\")\n",
    "\n",
    "# Filter out very short responses (likely errors)\n",
    "final_df = final_df[final_df[\"response\"].str.len() > 20]\n",
    "print(f\"[4/5] After filtering short responses: {len(final_df):,} pairs\")\n",
    "\n",
    "# Shuffle dataset\n",
    "final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"[5/5] Dataset shuffled\")\n",
    "\n",
    "print(f\"\\n‚úì Final dataset ready with {len(final_df):,} prompt-response pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Data Validation & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quality checks...\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total pairs: 5,155,414\n",
      "\n",
      "Prompt Statistics:\n",
      "  Mean length: 150.0 characters\n",
      "  Median length: 66.0 characters\n",
      "  Min length: 16 characters\n",
      "  Max length: 10132 characters\n",
      "\n",
      "Response Statistics:\n",
      "  Mean length: 673.5 characters\n",
      "  Median length: 533.0 characters\n",
      "  Min length: 21 characters\n",
      "  Max length: 16332 characters\n",
      "\n",
      "Quality Checks:\n",
      "  ‚úì No missing values: True\n",
      "  ‚úì No duplicates: True\n",
      "  ‚úì All prompts non-empty: True\n",
      "  ‚úì All responses non-empty: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Running quality checks...\\n\")\n",
    "\n",
    "# Calculate statistics\n",
    "prompt_lengths = final_df[\"prompt\"].str.len()\n",
    "response_lengths = final_df[\"response\"].str.len()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total pairs: {len(final_df):,}\")\n",
    "print(f\"\\nPrompt Statistics:\")\n",
    "print(f\"  Mean length: {prompt_lengths.mean():.1f} characters\")\n",
    "print(f\"  Median length: {prompt_lengths.median():.1f} characters\")\n",
    "print(f\"  Min length: {prompt_lengths.min()} characters\")\n",
    "print(f\"  Max length: {prompt_lengths.max()} characters\")\n",
    "print(f\"\\nResponse Statistics:\")\n",
    "print(f\"  Mean length: {response_lengths.mean():.1f} characters\")\n",
    "print(f\"  Median length: {response_lengths.median():.1f} characters\")\n",
    "print(f\"  Min length: {response_lengths.min()} characters\")\n",
    "print(f\"  Max length: {response_lengths.max()} characters\")\n",
    "\n",
    "# Check for issues\n",
    "print(f\"\\nQuality Checks:\")\n",
    "print(f\"  ‚úì No missing values: {final_df.isnull().sum().sum() == 0}\")\n",
    "print(f\"  ‚úì No duplicates: {final_df.duplicated().sum() == 0}\")\n",
    "print(f\"  ‚úì All prompts non-empty: {(final_df['prompt'].str.len() > 0).all()}\")\n",
    "print(f\"  ‚úì All responses non-empty: {(final_df['response'].str.len() > 0).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to datasets/Cleaned/clean_recipes.csv...\n",
      "\n",
      "================================================================================\n",
      "‚úì SUCCESS! Dataset saved successfully.\n",
      "================================================================================\n",
      "\n",
      "Output Details:\n",
      "  File: datasets/Cleaned/clean_recipes.csv\n",
      "  Size: 4073.08 MB\n",
      "  Rows: 5,155,414\n",
      "  Columns: 2\n",
      "\n",
      "Ready for:\n",
      "  ‚úÖ Embedding generation (Word2Vec, BERT, etc.)\n",
      "  ‚úÖ LSTM/RNN training\n",
      "  ‚úÖ Transformer fine-tuning (GPT-2, T5)\n",
      "  ‚úÖ RAG system integration\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(OUTPUT_PATH) / (1024**2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úì SUCCESS! Dataset saved successfully.\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput Details:\")\n",
    "print(f\"  File: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Rows: {len(final_df):,}\")\n",
    "print(f\"  Columns: {len(final_df.columns)}\")\n",
    "print(f\"\\nReady for:\")\n",
    "print(f\"  ‚úÖ Embedding generation (Word2Vec, BERT, etc.)\")\n",
    "print(f\"  ‚úÖ LSTM/RNN training\")\n",
    "print(f\"  ‚úÖ Transformer fine-tuning (GPT-2, T5)\")\n",
    "print(f\"  ‚úÖ RAG system integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Preview Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prompt-Response Pairs:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Pair 1]\n",
      "PROMPT:\n",
      "  i have these ingredients: 1 c. salad dressing, 1 c. sour cream, 1 (10 oz.) pkg. frozen spinach, thawed and well drained, 12 c. onion, chopped, 12 c. parsley, chopped, 1 tsp. salt, 12 tsp. pepper. what can i make?\n",
      "\n",
      "RESPONSE:\n",
      "  you could make stadium spinach dip. here are the instructions: 1. combine all ingredients mix well. 2. chill. 3. serve with vegetables carrot sticks, celery sticks, broccoli, cauliflower, etc. 4. makes 3 cups....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Pair 2]\n",
      "PROMPT:\n",
      "  how do i make asparagus puff ring?\n",
      "\n",
      "RESPONSE:\n",
      "  to make asparagus puff ring, you'll need: 34 cup water, 6 tablespoons butter, 34 cup all-purpose flour, 12 teaspoon salt, 3 large eggs, 14 cup grated parmesan cheese, divided, 1 pound fresh asparagus, cut into 1-inch pieces, 14 cup diced onion, 2 tablespoons butter, 2 tablespoons all-purpose flour, ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Pair 3]\n",
      "PROMPT:\n",
      "  i have these ingredients: 1 cup peanut butter morsels, 12 cup corn syrup, light, 2 tablespoons butter or margarine, 4 cups crisp rice cereal, 12 ounces chocolate (semi-sweet) semi-sweet morsels, 14 cup butter or margarine, 1 cup powdered sugar sifted, 2 tablespoons milk, 12 teaspoon vanilla extract. what can i make?\n",
      "\n",
      "RESPONSE:\n",
      "  you could make peanut butter and fudge bars. here are the instructions: 1. combine peanut butter morsels, corn syrup, and 2 tablespoons butter in top of a double boiler bring water to a boil. 2. cook over hot water until morsels melt stir well. 3. add cereal, and stir well. 4. press half the mixture...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Pair 4]\n",
      "PROMPT:\n",
      "  i have these ingredients: 5 cups torn curly endive, 2 cups watercress, 1 shallot, thinly sliced, 13 cup pecan halves, toasted, 14 cup pomegranate seeds, 14 cup olive oil, 1-12 tablespoons lemon juice, 1 teaspoon grated lemon zest, 18 teaspoon salt, 18 teaspoon pepper. what can i make?\n",
      "\n",
      "RESPONSE:\n",
      "  you could make winter endive salad. here are the instructions: 1. in a large bowl, combine the endive, watercress and shallot. sprinkle with pecans and pomegranate seeds. 2. in a small bowl, whisk the oil, lemon juice, lemon zest, salt and pepper. drizzle over salad serve immediately....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Pair 5]\n",
      "PROMPT:\n",
      "  how do i make pepperoni cake?\n",
      "\n",
      "RESPONSE:\n",
      "  to make pepperoni cake, you'll need: 1 small cake yeast or 1 pkg. dry yeast, 1 c. lukewarm water, 1 c. flour, 8 eggs, 2 c. chopped or grated pepperoni, 3 c. shredded cheeses (cheddar, mozzarella, provolone or your choice, use more than one kind), 2 c. flour. then follow these steps: 1. mix yeast, wa...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Prompt-Response Pairs:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Pair {i+1}]\")\n",
    "    print(f\"PROMPT:\\n  {final_df.iloc[i]['prompt']}\")\n",
    "    print(f\"\\nRESPONSE:\\n  {final_df.iloc[i]['response'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Generate Conversational Training Data (NEW)\n",
    "\n",
    "This section creates a **3rd dataset** for multi-turn conversational LSTM training.\n",
    "\n",
    "**What it does:**\n",
    "- Takes `clean_recipes.csv` + `nutrition_lookup.csv`\n",
    "- Generates multi-turn dialogue pairs\n",
    "- Adds nutritional reasoning\n",
    "- Outputs: `conversational_training_data.csv`\n",
    "\n",
    "**Objectives:**\n",
    "- Enables your LSTM to have natural conversations\n",
    "- Teaches follow-up questions (\"would you like to know...\")\n",
    "- Adds nutritional judgments (\"this is healthy because...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING CONVERSATIONAL TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration: Using 100,000 recipe sample\n",
      "   Estimated processing time: 20-30 minutes\n",
      "   Expected output: ~600,000 conversational pairs\n",
      "\n",
      "[1/5] Loading existing datasets...\n",
      "      ‚úì Loaded 5,155,414 total recipe pairs\n",
      "      ‚úì Sampled 50,000 recipes for processing\n",
      "      ‚ÑπÔ∏è  This reduces processing time to 20-30 minutes\n",
      "      ‚úì Loaded 230,133 nutrition entries\n",
      "\n",
      "[2/5] Extracting recipe information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:00<00:00, 58360.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úì Extracted 23,751 unique recipes\n",
      "\n",
      "[3/5] Merging with nutrition data...\n",
      "      ‚úì Recipes with nutrition: 9,921\n",
      "      ‚ö† Recipes without nutrition: 13,830\n",
      "      (Recipes without nutrition will still be included)\n",
      "\n",
      "[4/5] Generating conversational pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conversations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23751/23751 [00:00<00:00, 37255.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úì Generated 114,834 total pairs\n",
      "      ‚úì Removed 3,387 duplicates\n",
      "      ‚úì Final count: 111,447 unique pairs\n",
      "\n",
      "[5/5] Saving conversational training data...\n",
      "\n",
      "================================================================================\n",
      "‚úì SUCCESS! CONVERSATIONAL TRAINING DATA GENERATED\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Input recipes: 50,000\n",
      "  Total training pairs: 111,447\n",
      "  File size: 79.25 MB\n",
      "  Output location: datasets/Cleaned/conversational_training_data.csv\n",
      "\n",
      "Pair Types:\n",
      "  Multi-turn conversations: 56,897\n",
      "  Single-turn queries: 54,550\n",
      "\n",
      "This dataset is ready for:\n",
      "  ‚úÖ LSTM training on multi-turn conversations\n",
      "  ‚úÖ Learning conversational flow patterns\n",
      "  ‚úÖ Nutritional reasoning and judgments\n",
      "\n",
      "================================================================================\n",
      "SAMPLE CONVERSATIONAL PAIRS:\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "INPUT:\n",
      "  [INGREDIENTS] 1 kg chicken wings, 2 tablespoons sesame oil, 3 garlic cloves, 2 cm ginger, 14 cup soy sauce, 12 cup honey, 14 cup ketjap manis (sweet s...\n",
      "\n",
      "OUTPUT:\n",
      "  okay heres how you can make it: 1. cut chicken wings into 3 segments at joints, we use everything. 2. heat a deep wok for 1 minute on high. 3. pour oi...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 2]\n",
      "INPUT:\n",
      "  i have 2 boneless skinless chicken breasts, cooked and shredded, 15 ounces black beans, 1 cup corn, 12 small onion, chopped, 1 garlic clove, minced, 2...\n",
      "\n",
      "OUTPUT:\n",
      "  you could make black bean chicken quesadilla. here are the instructions: 1. combine first 6 ingredients and taco seasoning. 2. stir well to coat every...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 3]\n",
      "INPUT:\n",
      "  [INGREDIENTS] 9 cups old fashioned oats, 1 12 cups wheat germ, 1 12 cups grated unsweetened coconut, 13 cup sesame seeds, 12 cup shelled pumpkin seeds...\n",
      "\n",
      "OUTPUT:\n",
      "  i see youve got 9 cups old fashioned oats, 1 12 cups wheat germ, 1 12 cups grated unsweetened coconut, 13 cup sesame seeds, 12 cup shelled pumpkin see...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì You now have 3 datasets ready for LSTM training!\n",
      "  1. nutrition_lookup.csv (reference table)\n",
      "  2. clean_recipes.csv (single-turn training)\n",
      "  3. conversational_training_data.csv (multi-turn training)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING CONVERSATIONAL TRAINING DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚öôÔ∏è  Configuration: Using 100,000 recipe sample\")\n",
    "print(\"   Estimated processing time: 20-30 minutes\")\n",
    "print(\"   Expected output: ~600,000 conversational pairs\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_nutritional_judgment(calories, protein, fat, carbs, sugar):\n",
    "    \"\"\"Generate nutritional reasoning based on macronutrient values.\"\"\"\n",
    "    if pd.isna(calories) or calories == 0:\n",
    "        return \"this is a moderate option.\"\n",
    "    \n",
    "    if protein > 20 and fat < 15 and sugar < 10:\n",
    "        return \"this is a healthy choice as seen by the high protein content and low fat. its great for muscle building and satiety.\"\n",
    "    elif protein > 15 and fat < 25 and carbs < 50:\n",
    "        return \"this is a balanced meal with good protein and moderate macros. its suitable for most diets.\"\n",
    "    elif carbs > 50 and fat > 20:\n",
    "        return \"this is a nice treat to enjoy occasionally. its indulgent but can fit into a balanced diet in moderation.\"\n",
    "    elif calories < 200:\n",
    "        return \"this is a light and healthy option, perfect for a snack or side dish.\"\n",
    "    elif calories > 500:\n",
    "        return \"this is a hearty and filling meal. enjoy it when you need substantial energy.\"\n",
    "    elif carbs < 20 and fat > 30:\n",
    "        return \"this is a keto-friendly option with low carbs and high fat. great for low-carb diets.\"\n",
    "    elif sugar > 30:\n",
    "        return \"this is a sweet treat. enjoy it as a dessert or special occasion food.\"\n",
    "    else:\n",
    "        return \"this is a moderate option that fits most dietary needs.\"\n",
    "\n",
    "\n",
    "def create_conversational_pairs(row):\n",
    "    \"\"\"Create multi-turn conversation pairs from a single recipe.\"\"\"\n",
    "    ingredients = row.get('ingredients', '')\n",
    "    title = row.get('title', '')\n",
    "    directions = row.get('directions', '')\n",
    "    calories = row.get('calories', 0)\n",
    "    protein = row.get('protein_g', 0)\n",
    "    fat = row.get('fat_g', 0)\n",
    "    carbs = row.get('carbs_g', 0)\n",
    "    sugar = row.get('sugar_g', 0)\n",
    "    \n",
    "    if not title or not ingredients or not directions:\n",
    "        return []\n",
    "    \n",
    "    judgment = get_nutritional_judgment(calories, protein, fat, carbs, sugar)\n",
    "    pairs = []\n",
    "    \n",
    "    # Multi-turn conversation\n",
    "    turn1_input = f\"[INGREDIENTS] {ingredients}\"\n",
    "    turn1_output = f\"i see youve got {ingredients}. i would suggest making {title}. would you like to know how to make it?\"\n",
    "    pairs.append({\"input\": turn1_input, \"output\": turn1_output})\n",
    "    \n",
    "    turn2_input = f\"{turn1_input} [HISTORY] system: {turn1_output} user: yes\"\n",
    "    turn2_output = f\"okay heres how you can make it: {directions}. would you like to know the macronutrient information?\"\n",
    "    pairs.append({\"input\": turn2_input, \"output\": turn2_output})\n",
    "    \n",
    "    if not pd.isna(calories):\n",
    "        turn3_input = f\"{turn2_input} system: {turn2_output} user: yes\"\n",
    "        turn3_output = f\"this has {calories:.1f} calories, {protein:.1f}g protein, {fat:.1f}g fat, {carbs:.1f}g carbs, and {sugar:.1f}g sugar. {judgment}\"\n",
    "        pairs.append({\"input\": turn3_input, \"output\": turn3_output})\n",
    "    \n",
    "    # Single-turn variants\n",
    "    pairs.append({\"input\": f\"how do i make {title}?\", \"output\": f\"to make {title}, youll need: {ingredients}. then follow these steps: {directions}\"})\n",
    "    \n",
    "    if not pd.isna(calories):\n",
    "        pairs.append({\"input\": f\"what are the macronutrients in {title}?\", \"output\": f\"{title} has {calories:.1f} calories, {protein:.1f}g protein, {fat:.1f}g fat, {carbs:.1f}g carbs, and {sugar:.1f}g sugar. {judgment}\"})\n",
    "    \n",
    "    pairs.append({\"input\": f\"i have {ingredients}. what can i make?\", \"output\": f\"you could make {title}. here are the instructions: {directions}\"})\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA WITH SAMPLING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/5] Loading existing datasets...\")\n",
    "clean_recipes = pd.read_csv(\"datasets/Cleaned/clean_recipes.csv\")\n",
    "print(f\"      ‚úì Loaded {len(clean_recipes):,} total recipe pairs\")\n",
    "\n",
    "# Apply 50K sampling\n",
    "SAMPLE_SIZE = 50000\n",
    "if len(clean_recipes) > SAMPLE_SIZE:\n",
    "    clean_recipes = clean_recipes.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"      ‚úì Sampled {len(clean_recipes):,} recipes for processing\")\n",
    "    print(f\"      ‚ÑπÔ∏è  This reduces processing time to 20-30 minutes\")\n",
    "else:\n",
    "    print(f\"      ‚ÑπÔ∏è  Using all {len(clean_recipes):,} recipes (less than {SAMPLE_SIZE:,})\")\n",
    "\n",
    "try:\n",
    "    nutrition_lookup = pd.read_csv(\"datasets/Cleaned/nutrition_lookup.csv\")\n",
    "    print(f\"      ‚úì Loaded {len(nutrition_lookup):,} nutrition entries\")\n",
    "except FileNotFoundError:\n",
    "    print(\"      ‚ö† nutrition_lookup.csv not found, continuing without nutrition data\")\n",
    "    nutrition_lookup = None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: EXTRACT RECIPE INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/5] Extracting recipe information...\")\n",
    "recipe_data = []\n",
    "\n",
    "for _, row in tqdm(clean_recipes.iterrows(), total=len(clean_recipes), desc=\"Processing\"):\n",
    "    prompt = row['prompt']\n",
    "    response = row['response']\n",
    "    \n",
    "    if \"i have these ingredients:\" in prompt:\n",
    "        ingredients = prompt.split(\"i have these ingredients:\")[1].split(\".\")[0].strip()\n",
    "    elif \"i have\" in prompt and \"what can i make\" in prompt:\n",
    "        ingredients = prompt.split(\"i have\")[1].split(\"what can i make\")[0].strip().rstrip('.')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if \"you could make\" in response and \"here are the instructions:\" in response:\n",
    "        parts = response.split(\"you could make\")[1].split(\"here are the instructions:\")\n",
    "        if len(parts) == 2:\n",
    "            title = parts[0].strip().rstrip('.')\n",
    "            directions = parts[1].strip()\n",
    "            recipe_data.append({\"title\": title, \"ingredients\": ingredients, \"directions\": directions})\n",
    "    elif \"to make\" in response and \"you'll need:\" in response:\n",
    "        parts = response.split(\"to make\")[1].split(\"you'll need:\")\n",
    "        if len(parts) == 2:\n",
    "            title = parts[0].strip().rstrip(',')\n",
    "            rest = parts[1]\n",
    "            if \"then follow these steps:\" in rest:\n",
    "                ing_dir = rest.split(\"then follow these steps:\")\n",
    "                ingredients_alt = ing_dir[0].strip().rstrip('.')\n",
    "                directions = ing_dir[1].strip()\n",
    "                recipe_data.append({\"title\": title, \"ingredients\": ingredients_alt, \"directions\": directions})\n",
    "\n",
    "recipes_df = pd.DataFrame(recipe_data)\n",
    "print(f\"      ‚úì Extracted {len(recipes_df):,} unique recipes\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: MERGE WITH NUTRITION DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/5] Merging with nutrition data...\")\n",
    "if nutrition_lookup is not None:\n",
    "    merged = recipes_df.merge(nutrition_lookup, on='title', how='left')\n",
    "    with_nutrition = merged['calories'].notna().sum()\n",
    "    without_nutrition = merged['calories'].isna().sum()\n",
    "    print(f\"      ‚úì Recipes with nutrition: {with_nutrition:,}\")\n",
    "    print(f\"      ‚ö† Recipes without nutrition: {without_nutrition:,}\")\n",
    "    print(f\"      (Recipes without nutrition will still be included)\")\n",
    "else:\n",
    "    merged = recipes_df\n",
    "    merged['calories'] = np.nan\n",
    "    merged['protein_g'] = 0\n",
    "    merged['fat_g'] = 0\n",
    "    merged['carbs_g'] = 0\n",
    "    merged['sugar_g'] = 0\n",
    "    print(f\"      ‚ö† No nutrition data available\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: GENERATE CONVERSATIONAL PAIRS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/5] Generating conversational pairs...\")\n",
    "all_pairs = []\n",
    "for _, row in tqdm(merged.iterrows(), total=len(merged), desc=\"Creating conversations\"):\n",
    "    pairs = create_conversational_pairs(row)\n",
    "    all_pairs.extend(pairs)\n",
    "\n",
    "conversational_df = pd.DataFrame(all_pairs)\n",
    "initial_count = len(conversational_df)\n",
    "conversational_df = conversational_df.drop_duplicates()\n",
    "duplicates_removed = initial_count - len(conversational_df)\n",
    "conversational_df = conversational_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"      ‚úì Generated {initial_count:,} total pairs\")\n",
    "print(f\"      ‚úì Removed {duplicates_removed:,} duplicates\")\n",
    "print(f\"      ‚úì Final count: {len(conversational_df):,} unique pairs\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: SAVE OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/5] Saving conversational training data...\")\n",
    "output_path = \"datasets/Cleaned/conversational_training_data.csv\"\n",
    "conversational_df.to_csv(output_path, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_path) / (1024**2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì SUCCESS! CONVERSATIONAL TRAINING DATA GENERATED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Input recipes: {SAMPLE_SIZE:,}\")\n",
    "print(f\"  Total training pairs: {len(conversational_df):,}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Output location: {output_path}\")\n",
    "\n",
    "print(f\"\\nPair Types:\")\n",
    "multi_turn = conversational_df['input'].str.contains('[HISTORY]').sum()\n",
    "single_turn = len(conversational_df) - multi_turn\n",
    "print(f\"  Multi-turn conversations: {multi_turn:,}\")\n",
    "print(f\"  Single-turn queries: {single_turn:,}\")\n",
    "\n",
    "print(f\"\\nThis dataset is ready for:\")\n",
    "print(f\"  ‚úÖ LSTM training on multi-turn conversations\")\n",
    "print(f\"  ‚úÖ Learning conversational flow patterns\")\n",
    "print(f\"  ‚úÖ Nutritional reasoning and judgments\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREVIEW SAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE CONVERSATIONAL PAIRS:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(3, len(conversational_df))):\n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"INPUT:\")\n",
    "    print(f\"  {conversational_df.iloc[i]['input'][:150]}...\")\n",
    "    print(f\"\\nOUTPUT:\")\n",
    "    print(f\"  {conversational_df.iloc[i]['output'][:150]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n‚úì You now have 3 datasets ready for LSTM training!\")\n",
    "print(\"  1. nutrition_lookup.csv (reference table)\")\n",
    "print(\"  2. clean_recipes.csv (single-turn training)\")\n",
    "print(\"  3. conversational_training_data.csv (multi-turn training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Next Steps\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **Verify Outputs:** Inspect `clean_recipes.csv` and `conversational_training_data.csv`.\n",
    "2. **Create splits**: Split into train/validation/test sets\n",
    "3. **Generate embeddings**: Use Word2Vec, GloVe, or BERT\n",
    "\n",
    "### Model Training:\n",
    "1. **Dummy Model**: Build LSTM/RNN from scratch\n",
    "2. **Core Model**: Fine-tune GPT-2 or T5\n",
    "3. **Benchmark**: Compare against GPT-4/Claude\n",
    "\n",
    "### Integration:\n",
    "- Connect object detection outputs to LLM prompts\n",
    "- Implement RAG for context-aware responses\n",
    "- Add nutritional data extraction\n",
    "\n",
    "---\n",
    "\n",
    "Personal Notes:\n",
    "\n",
    "- Can we make the model more conversational by using scraping socials (e.g. reddit subreddits, twitter etc.)\n",
    "- Can we make the model more informed rather than hardcoding macronutrient sentiments?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
