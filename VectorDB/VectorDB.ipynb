{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb805d1",
   "metadata": {},
   "source": [
    "# Creating Vector DB for RAG Pipeline\n",
    "\n",
    "- **Authors:** Riyaadh Gani and Damilola Ogunleye\n",
    "- **Project:** Food Recognition & Recipe LLM  \n",
    "- **Purpose:** Creating VectorDB of recipe data and combining with RAG for the model\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook converts the RecipeNLG database into a vector data base using embeddings. This VectorDB can be queried by the LLM to add context to the recipes being provided to give better results.\n",
    "FAISS will be used to set up the vector DB\n",
    "\n",
    "**Output:** Functional model for recipe support: based on Recipe NLG data\n",
    "\n",
    "**Current Indexes:**\n",
    "- Small: Top 50,000 recipes in database\n",
    "- xsmol: 10000 recipes\n",
    "- recipe_index_10000.faiss\n",
    "- recipe_index_1000.faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cc5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (5.1.2)\n",
      "Requirement already satisfied: numpy in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: packaging in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu sentence-transformers numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383664e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dogun/Documents/UCL YEAR 3/deeplearning/deeplearnvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../datasets/Cleaned/clean_recipes_1000.csv')\n",
    "\n",
    "# find length of dataset and print it\n",
    "print(f\"Number of rows in dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6ae3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to first 50000 rows for faster processing\n",
    "# df = df.head(10000)\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f778199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n",
      "Memory estimate: 0.00 GB\n",
      "Processing batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 25.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check your dataset size first\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(f\"Memory estimate: {len(df) * 384 * 4 / 1e9:.2f} GB\")\n",
    "\n",
    "# Process in batches if too large\n",
    "def create_embeddings_batched(texts, batch_size=1000):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "        \n",
    "        batch_embeddings = model.encode(\n",
    "            batch,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Use batched approach\n",
    "embeddings = create_embeddings_batched(df['prompt'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7cb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1000, 384)\n",
      "Embeddings dtype: float32\n",
      "Memory usage: 0.00 GB\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n"
     ]
    }
   ],
   "source": [
    "# Check embeddings first\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings dtype: {embeddings.dtype}\")\n",
    "print(f\"Memory usage: {embeddings.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Contains NaN: {np.isnan(embeddings).any()}\")\n",
    "print(f\"Contains Inf: {np.isinf(embeddings).any()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d42211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing with sklearn...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Safer normalization\n",
    "print(\"Normalizing with sklearn...\")\n",
    "embeddings = embeddings.astype('float32')\n",
    "embeddings = normalize(embeddings, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54649f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use with FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "output_file = f'recipe_index_{len(df)}.faiss'\n",
    "faiss.write_index(index, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921e9b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Recipe 1 (similarity: 0.720) ---\n",
      "Prompt: i have these ingredients: 1 12 cups kluski egg noodles, uncooked, 1 cup shredded cabbage, 34 cup cub...\n",
      "Response: you could make vegetable soup wtih kluski noodles. here are the instructions: 1. in 5-quart saucepan stir together all the ingredients heat to boiling...\n",
      "\n",
      "--- Recipe 2 (similarity: 0.719) ---\n",
      "Prompt: i have these ingredients: 1 (15 ounce) can mandarin oranges, drained, 1 (16 ounce) bag mixed salad g...\n",
      "Response: you could make asian inspired salad. here are the instructions: 1. in sauce pan heat water to boiling and add in frozen soy beans, cook as directed on...\n",
      "\n",
      "--- Recipe 3 (similarity: 0.695) ---\n",
      "Prompt: i have these ingredients: 3 cups apples, diced, 3 cups onions, diced, 3 cups celery, diced, 1 tables...\n",
      "Response: you could make apple rice stuffing. here are the instructions: 1. saute apples, onions and celery in butter. 2. add prepared stuffing, rice and broth....\n"
     ]
    }
   ],
   "source": [
    "def retrieve_recipes(query, k=3):\n",
    "    \"\"\"Retrieve top-k most similar recipes\"\"\"\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Get results\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        results.append({\n",
    "            'prompt': df.iloc[idx]['prompt'],\n",
    "            'response': df.iloc[idx]['response'],\n",
    "            'similarity': float(score)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it\n",
    "query = \"I have chicken, rice, and vegetables. What can I make?\"\n",
    "results = retrieve_recipes(query, k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Recipe {i} (similarity: {result['similarity']:.3f}) ---\")\n",
    "    print(f\"Prompt: {result['prompt'][:100]}...\")\n",
    "    print(f\"Response: {result['response'][:150]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearnvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
